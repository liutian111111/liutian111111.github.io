---
title: "Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning"
collection: publications
permalink: /publication/2021-11-01-Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning
excerpt: 'Distantly supervised relation extraction is widely used in the construction of knowledge bases due to its high efficiency...'
date: 2021-11-01
paperurl: 'https://aclanthology.org/2021.emnlp-main.15.pdf'
citation: 'Lin X, **Liu T**, Jia W, et al. Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning[C]//Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 165-174.'
---
Distantly supervised relation extraction is widely used in the construction of knowledge bases due to its high efficiency. However, the automatically obtained instances are of low quality with numerous irrelevant words. In addition, the strong assumption of distant supervision leads to the existence of noisy sentences in the sentence bags. In this paper, we propose a novel Multi-Layer Revision Network (MLRN) which alleviates the effects of word-level noise by emphasizing inner-sentence correlations before extracting relevant information within sentences. Then, we devise a balanced and noise-resistant Confidence-based Multi-Instance Learning (CMIL) method to filter out noisy sentences as well as assign proper weights to relevant ones. Extensive experiments on two New York Times (NYT) datasets demonstrate that our approach achieves significant improvements over the baselines.

[Download paper here](https://aclanthology.org/2021.emnlp-main.15.pdf)

Recommended citation: Lin X, **Liu T**, Jia W, et al. Distantly Supervised Relation Extraction using Multi-Layer Revision Network and Confidence-based Multi-Instance Learning[C]//Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021: 165-174.