---
title: "Regularized Attentive Capsule Network for Overlapped Relation Extraction"
collection: publications
permalink: /publication/2020-12-01-Regularized Attentive Capsule Network for Overlapped Relation Extraction
excerpt: 'Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts...'
date: 2020-04-15
paperurl: 'https://aclanthology.org/2020.coling-main.562.pdf'
citation: '**Liu T**, Lin X, Jia W, et al. Regularized Attentive Capsule Network for Overlapped Relation Extraction[C]//Proceedings of the 28th International Conference on Computational Linguistics. 2020: 6388-6398.'
---
Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network (RA-CapNet) to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our model achieves significant improvements in relation extraction.

[Download paper here](https://aclanthology.org/2020.coling-main.562.pdf)

Recommended citation: **Liu T**, Lin X, Jia W, et al. Regularized Attentive Capsule Network for Overlapped Relation Extraction[C]//Proceedings of the 28th International Conference on Computational Linguistics. 2020: 6388-6398.